---
title: "Machine Learning11"
author: "Saket Rathi"
date: "June 29, 2016"
output: html_document
---

#Machine Learning Programme

#### Loading Required Packages


```{r,warning=FALSE}
library(caret)
library(randomForest)

```


####Read both training and test data 


```{r}
test <-read.csv("C:/Users/saket/Desktop/Data Science/coursera ass2/Machine Learning/pml-testing.csv")
train <- read.csv("C:/Users/saket/Desktop/Data Science/coursera ass2/Machine Learning/pml-training.csv")

```


#### Looking into the data dimension of Test and Train Data

```{r}
dim(train)
dim(test)
```



#### Values of columns


```{r,eval= FALSE}
head(train)
head(test)
```



#### From basic observation we can see we need to remove few variables from our analysis .Index, user_name,rawtimestamp1 and 2, Cvtd timestamp


```{r}
train<-train[,-c(1:6)]
```


#### still training set has 160-6 variables that are too many to make any practical inference

```{r}
   train<- train[ ,-nearZeroVar(train)]

```


#### We have got a good starting point but still we could futher reduce removing NA lets remove columnwith 90% values being NA

```{r}
limit <- length(train)*.90
index<- !apply(train,2,function(x) sum(is.na(x))> limit || sum(x=="")> limit)
train<-train[,index]

```

#### Looking into the data dimension of Test and Train Data

```{r}
dim(train)
dim(test)
```

#### Now we have reasonable number of predictotrs


#### Usually for better prediction mtry should be p/3-> 54/3= 18 .Due to computer harware limitation I have ran only for mtry =5 which gave me good estimate.With little error

```{r}

modelfit <- randomForest(classe~.,data=train,mtry=5)
modelfit

```


####prediction

```{r}
  test1<- predict(modelfit,test)
  test1

```
